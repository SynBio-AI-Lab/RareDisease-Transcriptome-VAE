import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import logging
import os
import sys

# --- ê²½ë¡œ ìë™ ì„¤ì • ---
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
INPUT_FILE = os.path.join(BASE_DIR, "data", "raw", "NCBI GEO", "GSE134900_normalized_expr.valerie_celiac.human.csv.gz")
OUTPUT_DIR = os.path.join(BASE_DIR, "data", "synthetic")
MODEL_DIR = os.path.join(BASE_DIR, "models")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(MODEL_DIR, exist_ok=True)

# --- í•˜ì´í¼íŒŒë¼ë¯¸í„° (íŠœë‹ ì™„ë£Œ) ---
BATCH_SIZE = 32
LEARNING_RATE = 1e-4
EPOCHS = 1000            # ìµœëŒ€ ì—í­ (Early Stoppingìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œ ê°€ëŠ¥)
HIDDEN_DIM = 512
LATENT_DIM = 64
KL_WEIGHT = 0.005        # latent space ì •ê·œí™” (0.001~0.01 ê¶Œì¥)
SEED = 42
SYNTHETIC_SAMPLES = 1000
VALIDATION_SPLIT = 0.2   # ê²€ì¦ ì„¸íŠ¸ ë¹„ìœ¨
EARLY_STOPPING_PATIENCE = 50  # ê²€ì¦ ì†ì‹¤ì´ ê°œì„ ë˜ì§€ ì•Šìœ¼ë©´ ì¡°ê¸° ì¢…ë£Œ
LR_SCHEDULER_PATIENCE = 30    # Learning Rate ê°ì†Œ patience
TEMPERATURE = 2.0             # ìƒì„± ì‹œ Latent Space íƒìƒ‰ ë²”ìœ„ í™•ëŒ€ (1.0=ê¸°ë³¸, 2.0=2ë°° ë¶„ì‚°)

torch.manual_seed(SEED)
np.random.seed(SEED)
logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(message)s')
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# --- ë°ì´í„° ë¡œë“œ ---
if not os.path.exists(INPUT_FILE):
    logging.error(f"âŒ íŒŒì¼ ì—†ìŒ: {INPUT_FILE}")
    sys.exit(1)

logging.info("ë°ì´í„° ë¡œë“œ ì¤‘...")
df = pd.read_csv(INPUT_FILE, index_col=0, compression='gzip')
if df.shape[0] > df.shape[1] and df.shape[0] > 1000:
    df = df.T
df = df.loc[:, ~df.columns.duplicated()] # ì¤‘ë³µ ì œê±°

scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(df.values)
input_dim = df.shape[1]

# Train/Validation ë¶„ë¦¬
train_data, val_data = train_test_split(data_scaled, test_size=VALIDATION_SPLIT, random_state=SEED)
logging.info(f"ğŸ“Š ë°ì´í„° ë¶„ë¦¬: í•™ìŠµ {len(train_data)}ê°œ, ê²€ì¦ {len(val_data)}ê°œ")

train_tensor = torch.FloatTensor(train_data).to(device)
val_tensor = torch.FloatTensor(val_data).to(device)
train_loader = DataLoader(TensorDataset(train_tensor), batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(TensorDataset(val_tensor), batch_size=BATCH_SIZE, shuffle=False)

# --- VAE ëª¨ë¸ (Batch Normalization ì ìš©) ---
class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        # Encoder
        self.fc1 = nn.Linear(input_dim, HIDDEN_DIM)
        self.bn1 = nn.BatchNorm1d(HIDDEN_DIM)
        self.fc2 = nn.Linear(HIDDEN_DIM, HIDDEN_DIM // 2)
        self.bn2 = nn.BatchNorm1d(HIDDEN_DIM // 2)
        self.fc21 = nn.Linear(HIDDEN_DIM // 2, LATENT_DIM)  # mu
        self.fc22 = nn.Linear(HIDDEN_DIM // 2, LATENT_DIM)  # logvar
        
        # Decoder
        self.fc3 = nn.Linear(LATENT_DIM, HIDDEN_DIM // 2)
        self.bn3 = nn.BatchNorm1d(HIDDEN_DIM // 2)
        self.fc4 = nn.Linear(HIDDEN_DIM // 2, HIDDEN_DIM)
        self.bn4 = nn.BatchNorm1d(HIDDEN_DIM)
        self.fc5 = nn.Linear(HIDDEN_DIM, input_dim)

    def encode(self, x):
        h = torch.relu(self.bn1(self.fc1(x)))
        h = torch.relu(self.bn2(self.fc2(h)))
        return self.fc21(h), self.fc22(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        h = torch.relu(self.bn3(self.fc3(z)))
        h = torch.relu(self.bn4(self.fc4(h)))
        return torch.sigmoid(self.fc5(h))  # [0,1] ë²”ìœ„ë¡œ ì œí•œ

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    MSE = nn.functional.mse_loss(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return MSE + (KL_WEIGHT * KLD)

# --- í•™ìŠµ ---
model = VAE().to(device)
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer, mode='min', factor=0.5, patience=LR_SCHEDULER_PATIENCE
)

# Early Stopping ë³€ìˆ˜
best_val_loss = float('inf')
patience_counter = 0
best_model_state = None

logging.info(f"ğŸš€ í•™ìŠµ ì‹œì‘ (ìµœëŒ€ {EPOCHS} Epochs, Early Stopping ì ìš©)...")

for epoch in range(EPOCHS):
    # --- í•™ìŠµ ë‹¨ê³„ ---
    model.train()
    train_loss = 0
    for data, in train_loader:
        optimizer.zero_grad()
        recon, mu, logvar = model(data)
        loss = loss_function(recon, data, mu, logvar)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()
    
    avg_train_loss = train_loss / len(train_loader.dataset)
    
    # --- ê²€ì¦ ë‹¨ê³„ ---
    model.eval()
    val_loss = 0
    with torch.no_grad():
        for data, in val_loader:
            recon, mu, logvar = model(data)
            loss = loss_function(recon, data, mu, logvar)
            val_loss += loss.item()
    
    avg_val_loss = val_loss / len(val_loader.dataset)
    
    # Learning Rate Scheduler ì—…ë°ì´íŠ¸
    scheduler.step(avg_val_loss)
    
    # Early Stopping ì²´í¬
    if avg_val_loss < best_val_loss:
        best_val_loss = avg_val_loss
        patience_counter = 0
        best_model_state = model.state_dict().copy()
    else:
        patience_counter += 1
    
    # ë¡œê¹… (50 ì—í­ë§ˆë‹¤ ë˜ëŠ” Early Stopping ì„ë°• ì‹œ)
    if (epoch + 1) % 50 == 0 or patience_counter >= EARLY_STOPPING_PATIENCE - 10:
        current_lr = optimizer.param_groups[0]['lr']
        logging.info(
            f'Epoch {epoch+1:4d} | Train Loss: {avg_train_loss:.4f} | '
            f'Val Loss: {avg_val_loss:.4f} | LR: {current_lr:.2e} | '
            f'Patience: {patience_counter}/{EARLY_STOPPING_PATIENCE}'
        )
    
    # Early Stopping ë°œë™
    if patience_counter >= EARLY_STOPPING_PATIENCE:
        logging.info(f"â¹ï¸ Early Stopping ë°œë™! (Epoch {epoch+1})")
        break

# ìµœì ì˜ ëª¨ë¸ ë³µì›
if best_model_state is not None:
    model.load_state_dict(best_model_state)
    logging.info(f"âœ… ìµœì  ëª¨ë¸ ë³µì› (Val Loss: {best_val_loss:.4f})")

# --- ëª¨ë¸ ì €ì¥ ---
model_path = os.path.join(MODEL_DIR, "vae_celiac.pt")
torch.save({
    'model_state_dict': model.state_dict(),
    'input_dim': input_dim,
    'scaler': scaler,
    'columns': df.columns.tolist()
}, model_path)
logging.info(f"âœ… ëª¨ë¸ ì €ì¥: {model_path}")

# --- í•©ì„± ë°ì´í„° ìƒì„± ---
model.eval()
with torch.no_grad():
    z = torch.randn(SYNTHETIC_SAMPLES, LATENT_DIM).to(device) * TEMPERATURE
    syn_scaled = model.decode(z).cpu().numpy()  # sigmoidë¡œ ì´ë¯¸ [0,1] ë²”ìœ„
    logging.info(f"ğŸŒ¡ï¸ Temperature {TEMPERATURE} ì ìš©í•˜ì—¬ Latent Space íƒìƒ‰ ë²”ìœ„ í™•ëŒ€")
    syn_data = scaler.inverse_transform(syn_scaled)
    
    output_path = os.path.join(OUTPUT_DIR, "synthetic_celiac_data.csv")
    pd.DataFrame(syn_data, columns=df.columns).to_csv(output_path, index=False)
    logging.info(f"âœ… í•©ì„± ë°ì´í„° ì €ì¥: {output_path} ({SYNTHETIC_SAMPLES}ê°œ ìƒ˜í”Œ)")

logging.info("ğŸ‰ ì™„ë£Œ!")